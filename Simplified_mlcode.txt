1. Array

import numpy as np
arr=np.array([[1,2,3],[4,5,6],[7,8,9]])
print("\nArray : \n",arr)
print("\nSize : ",arr.size)
print("\nDimension  : ",arr.ndim)
print("\nShape : ",arr.shape)
print("\nMean : ",np.mean(arr))
print("\nRow Vector : ",np.mean(arr,axis=0))
print("\nColumn Vector : ",np.mean(arr,axis=1))
print("\nMedian : ",np.median(arr))
print("\nVariance : ",np.var(arr))
print("\nStandard Deviation : ",np.std(arr))


# Mode

from scipy import stats as st
import numpy as np
c=np.array([5,2,2,2,4,6,7,7])
print("Mode : ",st.mode(c))


# cityblock and euclidean

from scipy.spatial.distance import cityblock,euclidean
a=[1,2,3,4]
b=[5,6,7,8]
print("Euclidean : ",euclidean(a,b))
print("City Block : ",cityblock(a,b))  


2. Dissimilarity Matrix

import numpy as np
from scipy.spatial.distance import cdist
data = np.random.rand(5, 5)
dissimilarity_matrix = cdist(data, data, metric='euclidean')
print("Dissimilarity Matrix (Euclidean Distance):")
print(dissimilarity_matrix)


#Binary vectors (Jacards similarity)

from sklearn.metrics import jaccard_score
vec1 = [0,1,0,0,0,1,0,0,1]
vec2 = [0,0,1,0,0,0,0,0,1]
similarity = jaccard_score(vec1, vec2)
print("Jaccards Similarity - Binary :", similarity)


# Non- Binary vectors or Cosine vectors (Jacards similarity)

import numpy as np
from scipy.spatial.distance import cosine
vec1 = [1, 2, 3, 4, 5]
vec2 = [2, 3, 4, 5, 6]
cosine_similarity = 1 - cosine(vec1, vec2)
print("Cosine Similarity:", cosine_similarity)



# Sets (Jacards similarity)

def jaccard_similarity_sets(set1, set2):
  intersection = set1.intersection(set2)
  union = set1.union(set2)
  return len(intersection) / len(union)
set1 = {1, 2, 3, 4}
set2 = {3, 4, 5, 6}
sets = jaccard_similarity_sets(set1, set2)
print("Sets similarity :",sets)



3. Normalization using Iris

import seaborn as sns
import numpy as np

iris=sns.load_dataset("iris")
x=iris.iloc[:,:-1]
print(x)
y=iris.iloc[:,-1]
print(y)

#1-Max Normalization
max1=x/(np.max(x,axis=0))
print("\n Max N:\n\n",max1)

#2-Min Max Normalization
min_max=(x-np.min(x,axis=0))/(np.max(x,axis=0)-np.min(x,axis=0))
print("\n Min Max N:\n\n",min_max)

#3-Upper Lower Normalization
u=20
l=10
max2=((x-np.min(x,axis=0))/(np.max(x,axis=0)-np.min(x,axis=0))*(u-l)+l)
print("\n Upper Lower N:\n\n",max2)

#Z-Score Normalization
z_score=(x-np.mean(x,axis=0))/(np.std(x,axis=0))
print("\n Z-Score N:\n\n",z_score)



4. Pearson

from scipy.stats import pearsonr
X = [1, 2, 3, 4, 5]
Y = [5, 4, 3, 2, 1]
result = pearsonr(X, Y)
corr = result[0]
print("Pearson Correlation Coefficient:",corr)





5. Simple Linear Regression for placement (3).csv dataset


import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_error

df=pd.read_csv(r'D:\NIE\3rd Sem\Machine Learning\ML Lab\placement (3).csv')
df

plt.scatter(df['cgpa'],df['package'])
plt.xlabel('CGPA')
plt.ylabel('Package')

x=df.iloc[:,0:1]
y=df.iloc[:,-1]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

lr=LinearRegression()
lr.fit(x_train,y_train)

y_pred=lr.predict(x_test.values)

plt.scatter(df['cgpa'],df['package'])
plt.plot(x_train,lr.predict(x_train),color='red')
plt.xlabel('cgpa')
plt.ylabel('package')

m=lr.coef_
print("Slope, m = ",m)

b=lr.intercept_
print("Y-Intercept, c = ",b)

m*8.58+b

MSE = mean_squared_error(y_test, y_pred)
print("Mean Squared Error is :",MSE)

MAE=mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error is :",MAE)

RMSE = np.sqrt(MSE)
print("Root Mean Squared Error is:", RMSE)






6. K-Means Clustring using Iris dataset

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans

df=sns.load_dataset('iris')

plt.scatter(df['sepal_length'],df['sepal_width'],df['petal_length'],df['petal_width'])

a=df.iloc[:,0:4]
b=df.iloc[:,-1]

wcss1=[]
for i in range(1,11):
    km=KMeans(n_clusters=i)
    km.fit_predict(a)
    wcss1.append(km.inertia_)

plt.plot(range(1,11),wcss1)

km=KMeans(n_clusters=4)
y_mean=km.fit_predict(a)
y_mean

a.iloc[y_mean==3,1]

plt.scatter(a.iloc[y_mean == 0, 0], a.iloc[y_mean == 0, 1], color='blue')
plt.scatter(a.iloc[y_mean == 1, 0], a.iloc[y_mean == 1, 1], color='red')
plt.scatter(a.iloc[y_mean == 2, 0], a.iloc[y_mean == 2, 1], color='green')
plt.scatter(a.iloc[y_mean == 3, 0], a.iloc[y_mean == 3, 1], color='yellow')
plt.show()





7. KNN Clustring using Mnist dataset

import numpy as np                               
from sklearn.datasets import fetch_openml        
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier       
from sklearn.metrics import classification_report,classification_report

mnist = fetch_openml('mnist_784', version=1)

x = mnist['data'].values
y = mnist['target'].astype(int).values

scaler = StandardScaler()                 
X_scaled = scaler.fit_transform(x)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print(f"CR : {classification_report(y_test, y_pred)}")

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)







8. ecision Tree Classifier with all 6 Pre-Pruning Technique

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix,classification_report

iris = load_iris()
X = iris.data[:, :2]
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = DecisionTreeClassifier(
    criterion='gini',       
    max_depth=4,            
    min_samples_split=5,   
    min_samples_leaf=2,     
    max_leaf_nodes=10,      
    min_impurity_decrease=0.01)

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print(f"CR : {classification_report(y_test, y_pred)}")

cm = confusion_matrix(y_test, y_pred)
print(f'Confusion Matrix:\n{cm}')

plt.figure(figsize=(16, 10))
plot_tree(clf, feature_names=iris.feature_names,
          class_names=iris.target_names, filled=True)
plt.title("Decision Tree Visualization")
plt.show()






9. Create a Artificial Neural Network to classify Churn Modeling dataset

from google.colab import files
upload=files.upload()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

df=pd.read_csv('/content/Churn_Modelling.csv')
df

df=pd.get_dummies(df,columns=['Geography','Gender'])
df

X=df.drop(columns=['Exited','Surname'])
Y=df['Exited']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

scaler=StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s=scaler.transform(X_test)

import tensorflow
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

model=Sequential()
model.add(Dense(15,activation='relu',input_dim=15))
model.add(Dense(15,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.summary()

opt = Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

history=model.fit(X_train_s,Y_train,epochs=50,validation_split=0.2)

y_log=model.predict(X_test_s)
y_pred=np.where(y_log>0.5,1,0)

print(f'Accuracy: {accuracy_score(Y_test, y_pred) * 100:.2f}%')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.xlabel('epoch')
plt.ylabel('loss')





10. Create a Artificial Neural Network to classify Mnist-data.

import matplotlib.pyplot as plt
import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten
from sklearn.metrics import accuracy_score

(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

X_train[0]

plt.imshow(X_train[0])

X_train = X_train/255
X_test = X_test/255

X_train[0]

model = Sequential()
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(128,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))
model.summary()

model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])

history = model.fit(X_train,y_train,epochs=25,validation_split=0.2)

y_pred = model.predict(X_test).argmax(axis=1)

accuracy=accuracy_score(Y_test,y_pred)
print(f"Accuracy is {accuracy*100:.2f}%")

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

model.predict(X_test[15].reshape(1,28,28)).argmax(axis=1)
plt.imshow(X_test[15])




11. Write a program to classify email as spam or ham using Multinomial Naive-bayes classifier

from google.colab import files
uploaded = files.upload()

from sklearn.naive_bayes import MultinomialNB 
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

email=pd.read_csv("/content/IMDB Dataset.csv")
email

email['sentiment'].fillna(-1, inplace=True)
email['sentiment']=email['sentiment'].map({'ham':0,'spam':1,-1:-1})
y=email['sentiment']

vectorizer=CountVectorizer(max_features=5000)
x=vectorizer.fit_transform(email['review']).toarray()

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

nbc=MultinomialNB()
nbc.fit(x_train,y_train)

y_pred=nbc.predict(x_test)

accuracy=accuracy_score(y_test,y_pred)
print(f"Accuracy is {accuracy*100:.2f}%")
















