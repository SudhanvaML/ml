1. Array

import numpy as np
arr=np.array([[1,2,3],[4,5,6],[7,8,9]])
print("\nArray : \n",arr)
print("\nSize : ",arr.size)
print("\nDimension  : ",arr.ndim)
print("\nShape : ",arr.shape)
print("\nMean : ",np.mean(arr))
print("\nRow Vector : ",np.mean(arr,axis=0))
print("\nColumn Vector : ",np.mean(arr,axis=1))
print("\nMedian : ",np.median(arr))
print("\nVariance : ",np.var(arr))
print("\nStandard Deviation : ",np.std(arr))


# Mode

from scipy import stats as st
import numpy as np
c=np.array([5,2,2,2,4,6,7,7])
print("Mode : ",st.mode(c))


# cityblock and euclidean

from scipy.spatial.distance import cityblock,euclidean
a=[1,2,3,4]
b=[5,6,7,8]
print("Euclidean : ",euclidean(a,b))
print("City Block : ",cityblock(a,b))  


2. Dissimilarity Matrix

import numpy as np
from scipy.spatial.distance import cdist
data = np.random.rand(5, 5)
dissimilarity_matrix = cdist(data, data, metric='euclidean')
print("Dissimilarity Matrix (Euclidean Distance):")
print(dissimilarity_matrix)


#Binary vectors (Jacards similarity)

from sklearn.metrics import jaccard_score
vec1 = [0,1,0,0,0,1,0,0,1]
vec2 = [0,0,1,0,0,0,0,0,1]
similarity = jaccard_score(vec1, vec2)
print("Jaccards Similarity - Binary :", similarity)


# Non- Binary vectors or Cosine vectors (Jacards similarity)

import numpy as np
from scipy.spatial.distance import cosine
vec1 = [1, 2, 3, 4, 5]
vec2 = [2, 3, 4, 5, 6]
cosine_similarity = 1 - cosine(vec1, vec2)
print("Cosine Similarity:", cosine_similarity)



# Sets (Jacards similarity)

def jaccard_similarity_sets(set1, set2):
  intersection = set1.intersection(set2)
  union = set1.union(set2)
  return len(intersection) / len(union)
set1 = {1, 2, 3, 4}
set2 = {3, 4, 5, 6}
sets = jaccard_similarity_sets(set1, set2)
print("Sets similarity :",sets)



3. Normalization using Iris

import seaborn as sns
import numpy as np

iris=sns.load_dataset("iris")
x=iris.iloc[:,:-1]
print(x)
y=iris.iloc[:,-1]
print(y)

#1-Max Normalization
max1=x/(np.max(x,axis=0))
print("\n Max N:\n\n",max1)

#2-Min Max Normalization
min_max=(x-np.min(x,axis=0))/(np.max(x,axis=0)-np.min(x,axis=0))
print("\n Min Max N:\n\n",min_max)

#3-Upper Lower Normalization
u=20
l=10
max2=((x-np.min(x,axis=0))/(np.max(x,axis=0)-np.min(x,axis=0))*(u-l)+l)
print("\n Upper Lower N:\n\n",max2)

#Z-Score Normalization
z_score=(x-np.mean(x,axis=0))/(np.std(x,axis=0))
print("\n Z-Score N:\n\n",z_score)



4. Pearson

from scipy.stats import pearsonr
X = [1, 2, 3, 4, 5]
Y = [5, 4, 3, 2, 1]
result = pearsonr(X, Y)
corr = result[0]
print("Pearson Correlation Coefficient:",corr)





5. Simple Linear Regression for placement (3).csv dataset

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
df=pd.read_csv(r'C:\Sudhanva M L\Sudu_MLLab All\placement (3).csv')
df

plt.scatter(df['cgpa'],df['package'])
plt.xlabel('CGPA')
plt.ylabel('Package')

x=df.iloc[:,0:1]
print(x)

y=df.iloc[:,-1]
print(y)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)
from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr.fit(x_train,y_train)
x_test
y_test

y_predict=lr.predict(x_test.values)
print(y_predict)

plt.scatter(df['cgpa'],df['package'])
plt.plot(x_train,lr.predict(x_train),color='red')
plt.xlabel('cgpa')
plt.ylabel('package')

m=lr.coef_
print("Slope, m = ",m)

b=lr.intercept_
print("Y-Intercept, c = ",b)

m*8.58+b

from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_squared_error

MSE = mean_squared_error(y_test, y_predict)
print("Mean Squared Error is :",MSE)

MAE=mean_absolute_error(y_test, y_predict)
print("Mean Absolute Error is :",MAE)

RMSE = mean_squared_error(y_test, y_predict)
print("Root Mean Squared Error is:", RMSE)







6. K-Means Clustring using Iris dataset

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

df=sns.load_dataset('iris')
#print("Shape is : ",df.shape)
df.head()

plt.scatter(df['sepal_length'],df['sepal_width'],df['petal_length'],df['petal_width'])

a=df.iloc[:,0:4]
#print(x)

b=df.iloc[:,-1]
#print(y)

from sklearn.cluster import KMeans
wcss1=[]
for i in range(1,11):
    km=KMeans(n_clusters=i)
    km.fit_predict(a)
    wcss1.append(km.inertia_)
wcss1

plt.plot(range(1,11),wcss1)

km=KMeans(n_clusters=4)
y_mean=km.fit_predict(a)
y_mean

a.iloc[y_mean==3,1]

plt.scatter(a.iloc[y_mean == 0, 0], a.iloc[y_mean == 0, 1], color='blue')
plt.scatter(a.iloc[y_mean == 1, 0], a.iloc[y_mean == 1, 1], color='red')
plt.scatter(a.iloc[y_mean == 2, 0], a.iloc[y_mean == 2, 1], color='green')
plt.scatter(a.iloc[y_mean == 3, 0], a.iloc[y_mean == 3, 1], color='yellow')
plt.show()






7. KNN Clustring using Mnist dataset

import numpy as np                               
from sklearn.datasets import fetch_openml        
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier       
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report

mnist = fetch_openml('mnist_784', version=1)

x = mnist['data'].values
y = mnist['target'].astype(int).values

scaler = StandardScaler()                 
X_scaled = scaler.fit_transform(x)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train, y_train)

KNeighborsClassifier()

y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

precision = precision_score(y_test, y_pred, average='weighted')
print(f'Precision: {precision*100:.2f}%')

recall = recall_score(y_test, y_pred, average='weighted')
print(f'Recall: {recall*100:.2f}%')

f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1-Score: {f1*100:.2f}%')

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)







8. ecision Tree Classifier with all 6 Pre-Pruning Technique

from sklearn.tree import export_text,DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score
from sklearn.metrics import f1_score,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

iris=load_iris()
X=iris.data[:,:2]
# print("Data : \n",X)
y=iris.target
#print("Target : \n",y)

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

#Normal Classifier
clf=DecisionTreeClassifier()

# 1.Max_depth
clf=DecisionTreeClassifier(max_depth=3)

#2. Minimum sample split
clf=DecisionTreeClassifier(min_samples_split=30)

#3.Minimum sample leaf
clf=DecisionTreeClassifier(min_samples_leaf=30)

#4.Maximum leaf nodes
clf=DecisionTreeClassifier(max_leaf_nodes=10)

#5.Minimum Impurity Decrease
clf=DecisionTreeClassifier(min_impurity_decrease=0.2)

#6.Maximum Features
clf=DecisionTreeClassifier(max_features=2)

clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)
print(y_pred)

accuracy=accuracy_score(y_test,y_pred)
print(f"Accuracy : {accuracy*100:.2f}")

precision = precision_score(y_test, y_pred, average='weighted')
print(f'Precision: {precision*100:.2f}%')

recall = recall_score(y_test, y_pred, average='weighted')
print(f'Recall: {recall*100:.2f}%')

f1 = f1_score(y_test, y_pred, average='weighted')
print(f'F1-Score: {f1*100:.2f}%')

cm = confusion_matrix(y_test, y_pred)
print(f'Confusion Matrix:\n{cm}')

x_min,x_max=X[:,0].min()-1,X[:,0].max()+1
y_min,y_max=X[:,1].min()-1,X[:,1].max()+1
xx,yy=np.meshgrid(np.arange(x_min,x_max,0.01),np.arange(y_min,y_max,0.01))

Z=clf.predict(np.c_[xx.ravel(),yy.ravel()])
Z=Z.reshape(xx.shape)

plt.figure(figure=(10,6))
plt.contourf(xx,yy,Z,alpha=0.3,cmap=plt.cm.RdYlBu)
plt.scatter(X_train[:,0],X_train[:,1],c=y_train,edgecolors='k',cmap=plt.cm.RdYlBu,label='TrainingData')
plt.scatter(X_test[:,0],X_test[:,1],c=y_test,edgecolors='k',cmap=plt.cm.cool,label='TestingData')
plt.title("Decision Tree Classifier (First 2 Features)")
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.colorbar(label="Class")
plt.legend()
plt.show()

from sklearn.tree import plot_tree
plt.figure(figsize=(16,10))
plot_tree(clf,feature_names=["Sepal Length","Sepal Width"],
          class_names=iris.target_names,filled=True,rounded=True,fontsize=10)
plt.title("Decision Tree Visualization")
plt.show()









9. Create a Artificial Neural Network to classify Churn Modeling dataset

from google.colab import files
upload=files.upload()

import numpy as np
import pandas as pd

df=pd.read_csv('/content/Churn_Modelling.csv')
df

df.info()

df['Geography'].value_counts()

df['Gender'].value_counts()

df=pd.get_dummies(df,columns=['Geography','Gender'])
df

X=df.drop(columns=['Exited','Surname'])
X

Y=df['Exited']
Y

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

X_train_s = scaler.fit_transform(X_train)
X_test_s=scaler.transform(X_test)

X_train_s

import tensorflow
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model=Sequential()
model.add(Dense(15,activation='relu',input_dim=15))
model.add(Dense(15,activation='relu'))
model.add(Dense(1,activation='sigmoid'))

model.summary()

from tensorflow.keras.optimizers import Adam
opt = Adam(learning_rate=0.001) # Adjust learning rate
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

print("Shape of X_train_s:", X_train_s.shape)

history=model.fit(X_train_s,Y_train,epochs=50,validation_split=0.2)

model.layers[0].get_weights()

y_log=model.predict(X_test_s)

y_pred=np.where(y_log>0.5,1,0)

from sklearn.metrics import accuracy_score
accuracy_score(Y_test,y_pred)

import matplotlib.pyplot as plt
history.history

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')







10. Create a Artificial Neural Network to classify Mnist-data.

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten

(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()

X_train.shape

Y_train.shape

Y_train

X_train[0]

import matplotlib.pyplot as plt
plt.imshow(X_train[0])

X_train = X_train/255
X_test = X_test/255

X_train[0]

model = Sequential()
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(128,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))

model.summary()

model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])

history = model.fit(X_train,y_train,epochs=25,validation_split=0.2)

y_prob = model.predict(X_test)

y_pred = y_prob.argmax(axis=1)

from sklearn.metrics import accuracy_score
accuracy=accuracy_score(Y_test,y_pred)
print(f"Accuracy is {accuracy*100:.2f}%")

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

model.predict(X_test[15].reshape(1,28,28)).argmax(axis=1)

plt.imshow(X_test[15])






11. Write a program to classify email as spam or ham using Multinomial Naive-bayes classifier

from google.colab import files
uploaded = files.upload()

from sklearn.naive_bayes import MultinomialNB 
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd

email=pd.read_csv("/content/IMDB Dataset.csv")
email

email['sentiment'].fillna(-1, inplace=True)
email['sentiment']=email['sentiment'].map({'ham':0,'spam':1,-1:-1})
y=email['sentiment']

from sklearn.feature_extraction.text import CountVectorizer
vectorizer=CountVectorizer(max_features=5000)
x=vectorizer.fit_transform(email['review']).toarray()
x

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

nbc=MultinomialNB()
nbc.fit(x_train,y_train)

y_pred=nbc.predict(x_test)

accuracy=accuracy_score(y_test,y_pred)
print(f"Accuracy is {accuracy*100:.2f}%")


















